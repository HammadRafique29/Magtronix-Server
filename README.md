#  **Magtronix** ğŸš€ Local AI Automation - Electron & Python

## ğŸ”¥ Introduction
Local AI Automation is an upcoming **powerful desktop application** designed to bring **AI-driven automation** to your local system. Unlike cloud-based services like **Make.com**, this tool will run entirely on your machine using **Electron, Python, and Docker**, ensuring complete privacy, security, and performance.

This repository is currently being prepared to provide project details, updates, and insights before deployment. Stay tuned for the official release! ğŸš€

## ğŸ¯ Key Features (Planned)
- **AI-Powered Automation** â€“ Integrate multiple AI tools into a single app
- **Chatbot** â€“ Locally hosted chatbot for seamless conversations
- **Ollama WebUI** â€“ A powerful AI interface for managing Ollama models
- **Heygen Automation** â€“ Automate Heygen video creation workflows
- **TTS & Whisper** â€“ Convert text to speech and transcribe audio effortlessly
- **Image Generation** â€“ Generate AI-driven images locally
- **Scenario-Based Automation** â€“ Create workflows and automate repetitive tasks
- **Server Scheduling** â€“ Schedule tasks to run on your local server even when offline

## ğŸ› ï¸ Tech Stack
- **Frontend:** Electron.js (for cross-platform desktop UI)
- **Backend:** Python (for AI processing and automation tasks)
- **Containerization:** Docker (to keep everything modular and isolated)

## ğŸ› ï¸ Installation
- Install Docker
- Install Ollama
- Install NPM, Node
- Install Python3.11.9
- Download ffmpeg and ADD TO PATH [Windows]


## Linux Server
-- ` sudo apt-get install -y xvfb `
-- OPTIONAL `Xvfb :99 -screen 0 1920x1080x24 &`
-- OPTIONAL `export DISPLAY=:99`

-- Install Chrome 133
-- Download `ChromeDriver` & place it in `_backend.features.chrome_driver`

## âš™ï¸ Configuration
- Add Docker on Startup
- Add Ollama on Startup

## ğŸ³ Installation Using Docker
- ghcr.io/open-webui/open-webui:main
- ghcr.io/coqui-ai/tts:latest

## GPU Based System:
- pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 --extra-index-url https://download.pytorch.org/whl/cu117

## ğŸ“¥ Pulling Ollama Models
- llama3.2:1b       [Ollama - Optional]
- deepseek-r1:1.5b  [Ollama - Optional]

## ğŸš€ First Time Execution
- python3 app.py
- ollama serve [make sure to add Ollama on startup]

## ğŸ”„ Normal Execution
- Run runner.py
- Run Magtronix

## ğŸ‘¨â€ğŸ’» For Developers
- Run  `[ python3 -m venv venv ]`
- Run  `[ source venv/bin/activate ,  ./venv/Scripts/activate ]`
- Run  `[ pip install -r requirements.txt ]`
- Run  `[ pip install --upgrade pip setuptools wheel ]`
- Run  `[ python3 app.py ]`

## ğŸ“· App Screenshots
We have been working hard on the UI, and here are some early previews:

![Screenshot from 2025-03-03 13-47-24](https://github.com/user-attachments/assets/70005e3a-95cf-4149-8055-b991da8e311c)
![Screenshot from 2025-03-03 14-22-02](https://github.com/user-attachments/assets/8288b1b8-7c2d-48dc-9a28-8a1dddf6dbd3)
![Screenshot from 2025-03-03 14-22-19](https://github.com/user-attachments/assets/ce89b42f-75f0-4fc0-8f80-8df1dd8f1e29)
![Screenshot from 2025-03-03 14-22-27](https://github.com/user-attachments/assets/ef561fe0-5636-453d-bef1-1c1d410231fd)

## ğŸ“¢ Stay Tuned
This project is actively in development. **A dashboard preview will be shared soon!** ğŸ‘€

Follow this repository for updates and be the first to experience **AI-driven automation on your local system!** ğŸš€

