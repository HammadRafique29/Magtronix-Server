#  **Magtronix** 🚀 Local AI Automation - Electron & Python

## 🔥 Introduction
Local AI Automation is an upcoming **powerful desktop application** designed to bring **AI-driven automation** to your local system. Unlike cloud-based services like **Make.com**, this tool will run entirely on your machine using **Electron, Python, and Docker**, ensuring complete privacy, security, and performance.

This repository is currently being prepared to provide project details, updates, and insights before deployment. Stay tuned for the official release! 🚀

## 🎯 Key Features (Planned)
- **AI-Powered Automation** – Integrate multiple AI tools into a single app
- **Chatbot** – Locally hosted chatbot for seamless conversations
- **Ollama WebUI** – A powerful AI interface for managing Ollama models
- **Heygen Automation** – Automate Heygen video creation workflows
- **TTS & Whisper** – Convert text to speech and transcribe audio effortlessly
- **Image Generation** – Generate AI-driven images locally
- **Scenario-Based Automation** – Create workflows and automate repetitive tasks
- **Server Scheduling** – Schedule tasks to run on your local server even when offline

## 🛠️ Tech Stack
- **Frontend:** Electron.js (for cross-platform desktop UI)
- **Backend:** Python (for AI processing and automation tasks)
- **Containerization:** Docker (to keep everything modular and isolated)

## 🛠️ Installation
- Install Docker
- Install Ollama
- Install NPM, Node
- Install Python3.11.9
- Download ffmpeg and ADD TO PATH [Windows]


## Linux Server
-- ` sudo apt-get install -y xvfb `
-- OPTIONAL `Xvfb :99 -screen 0 1920x1080x24 &`
-- OPTIONAL `export DISPLAY=:99`

-- Install Chrome 133
-- Download `ChromeDriver` & place it in `_backend.features.chrome_driver`

## ⚙️ Configuration
- Add Docker on Startup
- Add Ollama on Startup

## 🐳 Installation Using Docker
- ghcr.io/open-webui/open-webui:main
- ghcr.io/coqui-ai/tts:latest

## 📥 Pulling Ollama Models
- llama3.2:1b       [Ollama - Optional]
- deepseek-r1:1.5b  [Ollama - Optional]

## 🚀 First Time Execution
- python3 app.py
- ollama serve [make sure to add Ollama on startup]

## 🔄 Normal Execution
- Run runner.py
- Run Magtronix

## 👨‍💻 For Developers
- Run  `[ python3 -m venv venv ]`
- Run  `[ source venv/bin/activate ,  ./venv/Scripts/activate ]`
- Run  `[ pip install -r requirements.txt ]`
- Run  `[ pip install --upgrade pip setuptools wheel ]`
- Run  `[ python3 app.py ]`

## 📷 App Screenshots
We have been working hard on the UI, and here are some early previews:

![Screenshot from 2025-03-03 13-47-24](https://github.com/user-attachments/assets/70005e3a-95cf-4149-8055-b991da8e311c)
![Screenshot from 2025-03-03 14-22-02](https://github.com/user-attachments/assets/8288b1b8-7c2d-48dc-9a28-8a1dddf6dbd3)
![Screenshot from 2025-03-03 14-22-19](https://github.com/user-attachments/assets/ce89b42f-75f0-4fc0-8f80-8df1dd8f1e29)
![Screenshot from 2025-03-03 14-22-27](https://github.com/user-attachments/assets/ef561fe0-5636-453d-bef1-1c1d410231fd)

## 📢 Stay Tuned
This project is actively in development. **A dashboard preview will be shared soon!** 👀

Follow this repository for updates and be the first to experience **AI-driven automation on your local system!** 🚀

